{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to preprocess data\n",
    "def preprocess_data(data):\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        cleaned_item = {\n",
    "            \"asin\": item.get(\"asin\", None),\n",
    "            \"title\": item.get(\"title\", None),\n",
    "            \"features\": item.get(\"feature\", None),\n",
    "            \"description\": item.get(\"description\", None),\n",
    "            \"price\": item.get(\"price\", None),\n",
    "            \"brand\": item.get(\"brand\", None),\n",
    "            \"categories\": item.get(\"categories\", None)\n",
    "            \n",
    "        }\n",
    "        cleaned_data.append(cleaned_item)\n",
    "    return cleaned_data\n",
    "\n",
    "# Function to save preprocessed data as JSON\n",
    "def save_preprocessed_data(data, output_file):\n",
    "    with open(output_file, 'a', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "        file.write('\\n')  # Add newline after each batch\n",
    "\n",
    "# Load the Sampled Amazon dataset and preprocess the data\n",
    "json_file = 'Sampled_Amazon_eta.json'\n",
    "output_file = 'preprocessed_amazon_metadata.json'\n",
    "\n",
    "batch_size = 1000  # Define batch size\n",
    "preprocessed_data = []\n",
    "\n",
    "with open(json_file, 'r', encoding='utf-8') as file:\n",
    "    batch = []\n",
    "    for idx, line in enumerate(file):\n",
    "        item = json.loads(line)\n",
    "        batch.append(item)\n",
    "\n",
    "        # Check if batch size is reached\n",
    "        if len(batch) >= batch_size:\n",
    "            preprocessed_batch = preprocess_data(batch)\n",
    "            save_preprocessed_data(preprocessed_batch, output_file)\n",
    "            batch = []  # Reset batch\n",
    "\n",
    "    # Process the remaining items if any\n",
    "    if batch:\n",
    "        preprocessed_batch = preprocess_data(batch)\n",
    "        save_preprocessed_data(preprocessed_batch, output_file)\n",
    "\n",
    "print(\"Preprocessing completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "producer for Streaming Pipeline Setup\n",
    "Develop a producer application that streams the preprocessed data in real time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Kafka broker address\n",
    "bootstrap_servers = ['localhost:9092']\n",
    "\n",
    "# Kafka topic to which data will be produced\n",
    "topic = 'preprocessed_data'\n",
    "\n",
    "# Function to read preprocessed data from JSON file and stream it to Kafka\n",
    "def stream_preprocessed_data_to_kafka(producer, input_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            producer.send(topic, json.dumps(json.loads(line)).encode('utf-8'))\n",
    "            time.sleep(0.1)  # Adjust sleep time based on your throughput requirements\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create Kafka producer\n",
    "    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "\n",
    "    # Path to the preprocessed data JSON file\n",
    "    input_file = 'preprocessed_amazon_metadata.json'\n",
    "\n",
    "    # Stream preprocessed data to Kafka\n",
    "    stream_preprocessed_data_to_kafka(producer, input_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def stream_data(json_file, topic):\n",
    "    producer = KafkaProducer(bootstrap_servers='localhost:9092',\n",
    "                             value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
    "\n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            item = json.loads(line)\n",
    "            producer.send(topic, value=item)\n",
    "            time.sleep(0.1)  # Simulate streaming delay\n",
    "    producer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    json_file = 'preprocessed_amazon_metadata.json'\n",
    "    topic = 'amazon_data'\n",
    "    stream_data(json_file, topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consumer for Streaming Pipeline Setup\n",
    "Develop a producer application that streams the preprocessed data in real time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "# Kafka broker address\n",
    "bootstrap_servers = ['localhost:9092']\n",
    "\n",
    "# Kafka topic from which data will be consumed\n",
    "topic = 'preprocessed_data'\n",
    "\n",
    "# Function to consume data from Kafka\n",
    "def consume_preprocessed_data_from_kafka(consumer):\n",
    "    for message in consumer:\n",
    "        # Decode and print the message value\n",
    "        print(json.loads(message.value.decode('utf-8')))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create Kafka consumer\n",
    "    consumer = KafkaConsumer(topic, bootstrap_servers=bootstrap_servers)\n",
    "\n",
    "    # Consume preprocessed data from Kafka\n",
    "    consume_preprocessed_data_from_kafka(consumer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "def consume_data(topic):\n",
    "    consumer = KafkaConsumer(topic, bootstrap_servers='localhost:9092',\n",
    "                             group_id='group',\n",
    "                             value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n",
    "    for message in consumer:\n",
    "        print(message.value)  # Replace with your processing logic\n",
    "    consumer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    topic = 'amazon_data'\n",
    "    consume_data(topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consumer code for Frequent Itemset Mining\n",
    "Implement the Apriori algorithm in one consumer. There should be print\n",
    "statements showing real-time insights and associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class Apriori:\n",
    "    def __init__(self, min_support):\n",
    "        self.min_support = min_support\n",
    "        self.item_counts = {}\n",
    "        self.frequent_itemsets = []\n",
    "\n",
    "    def process_transaction(self, transaction):\n",
    "        for item in transaction:\n",
    "            if item in self.item_counts:\n",
    "                self.item_counts[item] += 1\n",
    "            else:\n",
    "                self.item_counts[item] = 1\n",
    "\n",
    "    def update_frequent_itemsets(self):\n",
    "        self.frequent_itemsets = [item for item, count in self.item_counts.items() if count >= self.min_support]\n",
    "\n",
    "    def print_frequent_itemsets(self):\n",
    "        print(\"Frequent Itemsets:\")\n",
    "        for itemset in self.frequent_itemsets:\n",
    "            print(itemset)\n",
    "\n",
    "def consume_data(topic, apriori):\n",
    "    with open('preprocessed_amazon_metadata.json', 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            transaction = json.loads(line)['categories']\n",
    "            apriori.process_transaction(transaction)\n",
    "            apriori.update_frequent_itemsets()\n",
    "            apriori.print_frequent_itemsets()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    min_support = 5  # Adjust this threshold as needed\n",
    "    apriori = Apriori(min_support)\n",
    "    topic = 'amazon_data'\n",
    "    consume_data(topic, apriori)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "producer code for Frequent Itemset Mining\n",
    "Implement the Apriori algorithm in one consumer. There should be print\n",
    "statements showing real-time insights and associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def stream_data(json_file, topic):\n",
    "    producer = KafkaProducer(bootstrap_servers='localhost:9092',\n",
    "                             value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
    "\n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            item = json.loads(line)\n",
    "            producer.send(topic, value=item)\n",
    "            time.sleep(0.1)  # Simulate streaming delay\n",
    "    producer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    json_file = 'preprocessed_amazon_metadata.json'\n",
    "    topic = 'amazon_data'\n",
    "    stream_data(json_file, topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCY consumer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "\n",
    "class PCY:\n",
    "    def __init__(self, min_support, hash_table_size):\n",
    "        self.min_support = min_support\n",
    "        self.hash_table_size = hash_table_size\n",
    "        self.item_counts = {}\n",
    "        self.pair_counts = [0] * hash_table_size\n",
    "        self.frequent_itemsets = []\n",
    "\n",
    "    def hash_function(self, item1, item2):\n",
    "        return (hash(item1) + hash(item2)) % self.hash_table_size\n",
    "\n",
    "    def process_transaction(self, transaction):\n",
    "        # Count single items\n",
    "        for item in transaction:\n",
    "            if item in self.item_counts:\n",
    "                self.item_counts[item] += 1\n",
    "            else:\n",
    "                self.item_counts[item] = 1\n",
    "\n",
    "        # Count pairs and update hash table\n",
    "        pairs = list(itertools.combinations(transaction, 2))\n",
    "        for pair in pairs:\n",
    "            hash_value = self.hash_function(pair[0], pair[1])\n",
    "            self.pair_counts[hash_value] += 1\n",
    "\n",
    "    def update_frequent_itemsets(self):\n",
    "        frequent_single_items = [item for item, count in self.item_counts.items() if count >= self.min_support]\n",
    "        frequent_pairs = []\n",
    "        for pair, count in zip(itertools.combinations(frequent_single_items, 2), self.pair_counts):\n",
    "            if count >= self.min_support:\n",
    "                frequent_pairs.append(pair)\n",
    "        self.frequent_itemsets = frequent_single_items + frequent_pairs\n",
    "\n",
    "    def print_frequent_itemsets(self):\n",
    "        print(\"Frequent Itemsets:\")\n",
    "        for itemset in self.frequent_itemsets:\n",
    "            print(itemset)\n",
    "\n",
    "def consume_data(topic, pcy):\n",
    "    with open('preprocessed_amazon_metadata.json', 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            transaction = json.loads(line)['categories']\n",
    "            pcy.process_transaction(transaction)\n",
    "            pcy.update_frequent_itemsets()\n",
    "            pcy.print_frequent_itemsets()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    min_support = 5  # Adjust this threshold as needed\n",
    "    hash_table_size = 10000  # Adjust hash table size as needed\n",
    "    pcy = PCY(min_support, hash_table_size)\n",
    "    topic = 'amazon_data'\n",
    "    consume_data(topic, pcy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pcy producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def stream_data(json_file, topic):\n",
    "    producer = KafkaProducer(bootstrap_servers='localhost:9092',\n",
    "                             value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
    "\n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            item = json.loads(line)\n",
    "            producer.send(topic, value=item)\n",
    "            time.sleep(0.1)  # Simulate streaming delay\n",
    "    producer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    json_file = 'preprocessed_amazon_metadata.json'\n",
    "    topic = 'amazon_data'\n",
    "    stream_data(json_file, topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "innovative and creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class RecommendationSystem:\n",
    "    def __init__(self, k_neighbors=5):\n",
    "        self.user_item_matrix = defaultdict(lambda: defaultdict(int))\n",
    "        self.k_neighbors = k_neighbors\n",
    "\n",
    "    def process_transaction(self, transaction):\n",
    "        user_id = transaction.get(\"user_id\")\n",
    "        items = transaction.get(\"items\", [])\n",
    "\n",
    "        for item in items:\n",
    "            self.user_item_matrix[user_id][item] += 1\n",
    "\n",
    "    def update_recommendations(self):\n",
    "        user_item_matrix = self.convert_matrix()\n",
    "        similarity_matrix = self.compute_similarity_matrix(user_item_matrix)\n",
    "        recommendations = self.generate_recommendations(user_item_matrix, similarity_matrix)\n",
    "        self.print_recommendations(recommendations)\n",
    "\n",
    "    def convert_matrix(self):\n",
    "        users = list(self.user_item_matrix.keys())\n",
    "        items = set()\n",
    "        for user_id, user_items in self.user_item_matrix.items():\n",
    "            items.update(user_items.keys())\n",
    "        items = list(items)\n",
    "\n",
    "        user_item_matrix = np.zeros((len(users), len(items)), dtype=np.int32)\n",
    "        for i, user_id in enumerate(users):\n",
    "            for j, item in enumerate(items):\n",
    "                user_item_matrix[i, j] = self.user_item_matrix[user_id][item]\n",
    "\n",
    "        return user_item_matrix\n",
    "\n",
    "    def compute_similarity_matrix(self, user_item_matrix):\n",
    "        similarity_matrix = cosine_similarity(user_item_matrix)\n",
    "        np.fill_diagonal(similarity_matrix, 0)  # Set diagonal elements to 0 to avoid self-similarity\n",
    "        return similarity_matrix\n",
    "\n",
    "    def generate_recommendations(self, user_item_matrix, similarity_matrix):\n",
    "        num_users = user_item_matrix.shape[0]\n",
    "        recommendations = defaultdict(list)\n",
    "\n",
    "        for i in range(num_users):\n",
    "            similar_users = np.argsort(similarity_matrix[i])[::-1][:self.k_neighbors]  # Get indices of top-k similar users\n",
    "            for similar_user in similar_users:\n",
    "                common_items = np.where(user_item_matrix[i] * user_item_matrix[similar_user] > 0)[0]\n",
    "                for item in common_items:\n",
    "                    if user_item_matrix[i, item] == 0:  # Recommend items not interacted by the user\n",
    "                        recommendations[i].append(item)\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def print_recommendations(self, recommendations):\n",
    "        print(\"Recommendations:\")\n",
    "        for user_id, recommended_items in recommendations.items():\n",
    "            print(f\"User {user_id} may be interested in: {recommended_items}\")\n",
    "\n",
    "def consume_data(topic, recommendation_system):\n",
    "    with open('preprocessed_user_data.json', 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            transaction = json.loads(line)\n",
    "            recommendation_system.process_transaction(transaction)\n",
    "            recommendation_system.update_recommendations()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    recommendation_system = RecommendationSystem(k_neighbors=5)\n",
    "    topic = 'user_data'\n",
    "    consume_data(topic, recommendation_system)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "integration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "from pymongo import MongoClient\n",
    "\n",
    "class PCY:\n",
    "    def __init__(self, min_support, hash_table_size, db_name, collection_name):\n",
    "        self.min_support = min_support\n",
    "        self.hash_table_size = hash_table_size\n",
    "        self.item_counts = {}\n",
    "        self.pair_counts = [0] * hash_table_size\n",
    "        self.frequent_itemsets = []\n",
    "\n",
    "        # MongoDB setup\n",
    "        self.client = MongoClient('mongodb://localhost:27017/')\n",
    "        self.db = self.client[db_name]\n",
    "        self.collection = self.db[collection_name]\n",
    "\n",
    "    def hash_function(self, item1, item2):\n",
    "        return (hash(item1) + hash(item2)) % self.hash_table_size\n",
    "\n",
    "    def process_transaction(self, transaction):\n",
    "        # Count single items\n",
    "        for item in transaction:\n",
    "            if item in self.item_counts:\n",
    "                self.item_counts[item] += 1\n",
    "            else:\n",
    "                self.item_counts[item] = 1\n",
    "\n",
    "        # Count pairs and update hash table\n",
    "        pairs = list(itertools.combinations(transaction, 2))\n",
    "        for pair in pairs:\n",
    "            hash_value = self.hash_function(pair[0], pair[1])\n",
    "            self.pair_counts[hash_value] += 1\n",
    "\n",
    "    def update_frequent_itemsets(self):\n",
    "        frequent_single_items = [item for item, count in self.item_counts.items() if count >= self.min_support]\n",
    "        frequent_pairs = []\n",
    "        for pair, count in zip(itertools.combinations(frequent_single_items, 2), self.pair_counts):\n",
    "            if count >= self.min_support:\n",
    "                frequent_pairs.append(pair)\n",
    "        self.frequent_itemsets = frequent_single_items + frequent_pairs\n",
    "\n",
    "    def store_frequent_itemsets(self):\n",
    "        self.collection.insert_one({\"frequent_itemsets\": self.frequent_itemsets})\n",
    "\n",
    "    def print_frequent_itemsets(self):\n",
    "        print(\"Frequent Itemsets:\")\n",
    "        for itemset in self.frequent_itemsets:\n",
    "            print(itemset)\n",
    "\n",
    "def consume_data(topic, pcy):\n",
    "    with open('preprocessed_amazon_metadata.json', 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            transaction = json.loads(line)['categories']\n",
    "            pcy.process_transaction(transaction)\n",
    "            pcy.update_frequent_itemsets()\n",
    "            pcy.print_frequent_itemsets()\n",
    "            pcy.store_frequent_itemsets()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    min_support = 5  # Adjust this threshold as needed\n",
    "    hash_table_size = 10000  # Adjust hash table size as needed\n",
    "    db_name = 'streaming_db'\n",
    "    collection_name = 'frequent_itemsets'\n",
    "    pcy = PCY(min_support, hash_table_size, db_name, collection_name)\n",
    "    topic = 'amazon_data'\n",
    "    consume_data(topic, pcy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consumer2 a bit good "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "class PCY:\n",
    "    def __init__(self, min_support, hash_table_size):\n",
    "        self.min_support = min_support\n",
    "        self.hash_table_size = hash_table_size\n",
    "        self.item_counts = {}\n",
    "        self.pair_counts = {}\n",
    "        self.frequent_items = set()\n",
    "        self.frequent_pairs = set()\n",
    "\n",
    "    def process_transaction(self, transaction):\n",
    "        if transaction:\n",
    "            for item in transaction:\n",
    "                self.item_counts[item] = self.item_counts.get(item, 0) + 1\n",
    "\n",
    "    def update_frequent_items(self):\n",
    "        self.frequent_items = {item for item, count in self.item_counts.items() if count >= self.min_support}\n",
    "\n",
    "    def hash_function(self, pair):\n",
    "        return hash(pair) % self.hash_table_size\n",
    "\n",
    "    def process_pairs(self, transaction):\n",
    "        if transaction:\n",
    "            pairs = itertools.combinations(sorted(transaction), 2)\n",
    "            for pair in pairs:\n",
    "                if all(item in self.frequent_items for item in pair):\n",
    "                    hash_value = self.hash_function(pair)\n",
    "                    self.pair_counts[hash_value] = self.pair_counts.get(hash_value, 0) + 1\n",
    "\n",
    "    def update_frequent_pairs(self):\n",
    "        self.frequent_pairs = {(item1, item2) for (item1, item2), count in self.pair_counts.items() if isinstance(item1, tuple) and count >= self.min_support}\n",
    "\n",
    "    def print_frequent_itemsets(self):\n",
    "        print(\"Frequent Itemsets:\")\n",
    "        for itemset in self.frequent_items:\n",
    "            print(itemset)\n",
    "        print(\"Frequent Pairs:\")\n",
    "        for pair in self.frequent_pairs:\n",
    "            print(pair)\n",
    "\n",
    "def consume_data(topic, pcy):\n",
    "    consumer = KafkaConsumer(topic, bootstrap_servers=['localhost:9092'])\n",
    "\n",
    "    for message in consumer:\n",
    "        data = json.loads(message.value)\n",
    "        alsobuy_items = data.get('alsobuy')\n",
    "        pcy.process_transaction(alsobuy_items)\n",
    "        pcy.process_pairs(alsobuy_items)\n",
    "        pcy.update_frequent_items()\n",
    "        pcy.update_frequent_pairs()\n",
    "        pcy.print_frequent_itemsets()\n",
    "\n",
    "    consumer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    min_support = 5  # Adjust this threshold as needed\n",
    "    hash_table_size = 1000  # Adjust hash table size as needed\n",
    "    pcy = PCY(min_support, hash_table_size)\n",
    "    topic = 'preprocessed_data'\n",
    "    consume_data(topic, pcy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
